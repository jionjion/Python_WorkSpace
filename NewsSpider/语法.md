# Xpath语法
    article                                 选取所有的<article>元素的所有子节点
    /article                                选取根元素<article>
    article/a                               选取所有属于<article>的直接子元素的<a>元素
    //div                                   选取所有<div>子元素,无论出现在文档的任何位置
    article//div                            选取所有属于<article>元素的后代的div元素,不管它出现在<article>下的任何位置
    //@class                                选取所有名为class的属性,@表示选择属性
    /article/div[1]                         选取<article>子元素的第一个<div>元素,下标从1开始
    /article/div[last()-1]                  选取<article>子元素的倒数第一个<div>元素
    //div[@lang]                            选择所有拥有lang属性的<div>元素
    //div[@lang='en']                       选取所有lang属性值为en的<div>元素
    //div[contains(@class,"main")]          选取所有class含有main属性值的<div>元素
    //div[not(contains(@class,"main"))]     选取所有class不含有main属性值的<div>元素
    //div[starts-with(@class,"first")]      选取所有class以first开头的属性的<div>元素                 
    /div/*                                  选取属于<div>元素的所有子节点
    //*                                     选取所有元素
    //div[@*]                               选取带有属性的<div>元素
    /div/a | //div/p                        选取<div>元素下的<a>和<p>元素
    //span | //ul                           选取所有的<span>和<ul>元素
    article/div/p | //span                  选取所有的<article>元素的<div元素的<p>元素,以及文档中的<span>元素
    //html                                  选取整个Html页面
    /a/@href                                选取<a>链接的href属性值
    /a/text()                               选取<a>链接的标签内文本
    //*[text()="Hello"]                     选取所有标签内含有Hello内容的标签
    
    
    

在浏览器控制台中,可以用过`$x('Xpath')`方式进行解析

## 创建项目
在当前文件夹下,创建一个新项目
`scrapy startproject 项目名称`

## 创建爬虫
在当前项目下,创建一个爬虫
`scrapy genspider 爬虫名称 爬取网页URL`

# shell解析
爬取一个页面,并解析
`scrapy shell 解析网页URL`

## 准备

### 伪装用户浏览器
携带用户浏览器信息的请求
`scrapy shell -s USER_AGENT="Mozilla/5.0 (Windows NT 10.0; …) Gecko/20100101 Firefox/60.0" 解析网页URL`
### 忽略robots协议
在setting.py里把robots协议项设置为Flase,这样爬虫就不遵守网站规定的哪些不能爬取的协议
`ROBOTSTXT_OBEY = False`

scrapy shell -s USER_AGENT="Mozilla/5.0 (Windows NT 10.0; …) Gecko/20100101 Firefox/60.0" http://top.baidu.com/buzz?b=1&fr=topindex

### 解析语法

获取标签
response.xpath('//table[@class="list-table"]//td[@class="keyword"]/a[1]')

获得标签内容
response.xpath('//table[@class="list-table"]//td[@class="keyword"]/a[1]/text()')

获取标签本文
response.xpath('//table[@class="list-table"]//td[@class="keyword"]/a[1]/text()').extract()

获取内容,并追加正则
response.xpath('//table[@class="list-table"]//td[@class="keyword"]/a[1]/text()').re('[.0-9]+')



## ORM实体类
在`items.py`中通过继承编写实体,实体类以`Item`结尾
```python
class 对象Item(scrapy.Item):
    属性 = scrapy.Field()
```

在解析方法中,封装ORM类,并将封装后的类,通过`yield 对象`将其传递给pipeline入库

## 入库
在`pipelines.py`中编写不同的处理方法,可以将处理后的结果存入数据库或者文件
可以定义不同的类,在类名以`Pipeline`结尾,在方法中重写`process_item`对传递的ORM类进行操作
注意,编写完成的类需要在`setting.py`中注册才能使用该方法进行后续操作
 
 