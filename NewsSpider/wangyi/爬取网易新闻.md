# 基础知识
## xpath语法
    article                     选取所有的<article>元素的所有子节点
    /article                    选取根元素<article>
    article/a                   选取所有属于<article>的直接子元素的<a>元素
    //div                       选取所有<div>子元素,无论出现在文档的任何位置
    article//div                选取所有属于<article>元素的后代的div元素,不管它出现在<article>下的任何位置
    //@class                    选取所有名为class的属性,@表示选择属性
    /article/div[1]             选取<article>子元素的第一个<div>元素,下标从1开始
    /article/div[last()-1]      选取<article>子元素的倒数第一个<div>元素
    //div[@lang]                选择所有拥有lang属性的<div>元素
    //div[@lang='en']           选取所有lang属性值为en的<div>元素
    //div[contains(@class,"main")] 选取所有class含有main属性值的<div>元素
    /div/*                      选取属于<div>元素的所有子节点
    //*                         选取所有元素
    //div[@*]                   选取带有属性的<div>元素
    /div/a | //div/p            选取<div>元素下的<a>和<p>元素
    //span | //ul               选取所有的<span>和<ul>元素
    article/div/p | //span      选取所有的<article>元素的<div元素的<p>元素,以及文档中的<span>元素
    
    
# 爬取网易
## 创建项目
在文件夹下,执行,生成一个项目
`scrapy startproject wangyi`

## 爬取网易云跟帖(不会) 
### 创建根爬取页面
在spiders文件下,执行,会创建一个 wang_yi_gen_tie.py 爬虫文件在spiders文件夹下.
`scrapy genspider wang_yi_gen_tie http://tie.163.com/#/splendid`

当前可以使用的模板,生成爬虫文件
`scrapy genspider -l`

### 启动项目
启动爬虫,wang_yi_gen_tie
`scrapy crawl wang_yi_gen_tie`
启动爬虫,wang_yi_gen_tie,并将输出到文件items.json中,当然也可以是其他网络位置
通过后缀名.json,框架会自动将输出调整为JSON格式,类似的还有`.jl`,`.csv`,`.xml`格式
`scrapy crawl wang_yi_gen_tie -o items.json`

### 检查项目
检查爬虫是否正确
`scrapy check wang_yi_gen_tie` 

### 使用Shell脚本
scrapy shell http://tie.163.com/#/splendid


## 爬取网易新闻排行榜
### 创建根爬取页面
在spiders文件下,执行,会创建一个 wang_yi_xin_wen_pai_hang.py 文件在spiders文件夹下.
`scrapy genspider wang_yi_xin_wen_pai_hang http://news.163.com/rank/`

### 使用Shell脚本,解析
`scrapy shell http://news.163.com/rank/`

获得id为whole的div的下级h2标签内的文本
`response.xpath('//div[@id="whole"]/h2/text()')`


### 启动项目,注意名称要求一致
scrapy crawl wang_yi_xin_wen_pai_hang

